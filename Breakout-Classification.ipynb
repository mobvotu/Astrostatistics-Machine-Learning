
{
 "metadata": {
  "name": "",
  "signature": "sha256:024628f96f6dce5ffc22f9a799cac725908533b1cce78b87a9ac904e65e4bca5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Exploring Supervised Machine Learning: Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook explores a classification task on astronomical data with Scikit-Learn.\n",
      "Much of the practice of machine learning relies on searching through documentation to learn (or remind yourself) of how things are done. Recall that in IPython, you can see the documentation of any function using the ``?`` character. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.linspace?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In addition, Google is your friend. Do you want to know how to import a Gaussian mixture model in scikit-learn? Search for \"scikit-learn Gaussian Mixture\" and go from there."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we'll look at a *stellar photometry* dataset for classification, where the labels distinguish between variable and non-variable stars (based on multiple observations for the training set).\n",
      "\n",
      "(see also the [Regression breakout](Breakout-Regression.ipynb))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll start with the normal notebook imports & preliminaries:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# set matplotlib figure style\n",
      "mpl.style.use('ggplot') # only works in matplotlib 1.4+\n",
      "mpl.rc('figure', figsize=(8, 6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Data: Stellar Photometry\n",
      "\n",
      "The stellar photometry data is a combination of photometry for RR Lyrae and main sequence stars from SDSS Stripe 82, with RR Lyrae flagged based on their temporal variation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from astroML.datasets import fetch_rrlyrae_combined\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X, y = fetch_rrlyrae_combined()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data consist of 93141 objects, each with four features representing [u-g, g-r, r-i, i-z] colors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X.shape)\n",
      "print(X[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two labels, 0 = non-variable, 1 = variable star."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(y.shape)\n",
      "print(np.unique(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data have a large imbalance: that is, there are many more background stars than there are RR Lyrae stars.\n",
      "This is important to keep in mind when performing a machine learning task on them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(y == 0), np.sum(y == 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we can plot the data to better understand what we're looking at:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:, 0], X[:, 1], c=y,\n",
      "            linewidth=0, alpha=0.3)\n",
      "plt.xlabel('u-g')\n",
      "plt.ylabel('g-r')\n",
      "plt.axis('tight');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification: Labeling Variable Sources\n",
      "\n",
      "Here we'll explore a classification task using the variable sources shown above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. Explore the Data\n",
      "\n",
      "It's good to start by visually exploring the data you're working with.\n",
      "Plot various combinations of the inputs: if you were doing a classic manual separation of the data based on this, what would you do? Treating the variable stars as your \"signal\" and the rest as your \"background\", roughly what would you expect for the completeness and contamination of such a method?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. Applying a Fast & Simple Model\n",
      "\n",
      "Import the Gaussian Naive Bayes estimator from scikit-learn (try a Google search \u2013 the sklearn documentation will pop up, and you should be able to find the correct import statement pretty easily). This is a particularly useful estimator as a first-pass: it essentially fits each input class with an axis-aligned normal distribution, and thus is very fast (though usually not very accurate).\n",
      "\n",
      "Fit this model to the data, and then use cross-validation to determine the accuracy of the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the typical accuracy score may not be relevant here: if our goal is to find RR Lyrae from the background, you'll want to use a different scoring method (the ``scoring`` argument to ``cross_val_score``).\n",
      "The default scoring here is ``\"accuracy\"``, which basically means\n",
      "\n",
      "$$\n",
      "{\\rm precision} \\equiv \\frac{\\rm # correct labels}{\\rm total labels}\n",
      "$$\n",
      "\n",
      "But for unbalanced data, accuracy is actually a really bad metric! You can create a very accurate classifier by simply returning 0 for everything: the result will have an accuracy of $(93141 - 483) / 93141 = 99.5%$\n",
      "Needless to say, this model isn't very useful.\n",
      "\n",
      "Instead, we should look at the ``\"precision\"`` and ``\"recall\"`` scores.\n",
      "\n",
      "This is what the terms mean:\n",
      "\n",