
{
 "metadata": {
  "name": "",
  "signature": "sha256:f5a85bf5bf354025ca4bbae625d8e4bde4d0936513cee5d134e347f0c898433f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2015. Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2015/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Dimensionality Reduction: Principal Component Analysis in-depth\n",
      "\n",
      "Here we'll explore **Principal Component Analysis**, which is an extremely useful linear dimensionality reduction technique.\n",
      "\n",
      "We'll start with our standard set of initial imports:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division\n",
      "\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# use seaborn plotting style defaults\n",
      "import seaborn as sns; sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introducing Principal Component Analysis\n",
      "\n",
      "Principal Component Analysis is a very powerful unsupervised method for *dimensionality reduction* in data.  It's easiest to visualize by looking at a two-dimensional dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
      "plt.plot(X[:, 0], X[:, 1], 'o')\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that there is a definite trend in the data. What PCA seeks to do is to find the **Principal Axes** in the data, and explain how important those axes are in describing the data distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "print(pca.explained_variance_ratio_)\n",
      "print(pca.components_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see what these numbers mean, let's view them as vectors plotted on top of the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",
      "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \"important\" than the other direction.\n",
      "The explained variance quantifies this measure of \"importance\" in direction.\n",
      "\n",
      "Another way to think of it is that the second principal component could be **completely ignored** without much loss of information! Let's see what our data look like if we only keep 95% of the variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = PCA(0.95) # keep 95% of variance\n",
      "X_trans = clf.fit_transform(X)\n",
      "print(X.shape)\n",
      "print(X_trans.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_new = clf.inverse_transform(X_trans)\n",
      "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
      "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The light points are the original data, while the dark points are the projected version.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \"most important\" features of the data are maintained, and we've compressed the data by 50%!\n",
      "\n",
      "This is the sense in which \"dimensionality reduction\" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application of PCA to Digits\n",
      "\n",
      "The dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data.  Let's take a quick look at the application of PCA to the digits data we looked at before:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "X = digits.data\n",
      "y = digits.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(0.5)  # project from 64 to 2 dimensions\n",
      "Xproj = pca.fit_transform(X)\n",
      "print(X.shape)\n",
      "print(Xproj.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n",
      "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
      "plt.colorbar();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This gives us an idea of the relationship between the digits. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits, **without reference** to the labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What do the Components Mean?\n",
      "\n",
      "PCA is a very useful dimensionality reduction algorithm, because it has a very intuitive interpretation via *eigenvectors*.\n",
      "The input data is represented as a vector: in the case of the digits, our data is\n",
      "\n",
      "$$\n",
      "x = [x_1, x_2, x_3 \\cdots]\n",
      "$$\n",
      "\n",
      "but what this really means is\n",
      "\n",
      "$$\n",
      "image(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots\n",
      "$$\n",
      "\n",
      "If we reduce the dimensionality in the pixel space to (say) 6, we recover only a partial image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from fig_code.figures import plot_image_components\n",
      "\n",
      "sns.set_style('white')\n",
      "plot_image_components(digits.data[0])"