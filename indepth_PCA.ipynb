
{
 "metadata": {
  "name": "",
  "signature": "sha256:f5a85bf5bf354025ca4bbae625d8e4bde4d0936513cee5d134e347f0c898433f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2015. Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2015/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Dimensionality Reduction: Principal Component Analysis in-depth\n",
      "\n",
      "Here we'll explore **Principal Component Analysis**, which is an extremely useful linear dimensionality reduction technique.\n",
      "\n",
      "We'll start with our standard set of initial imports:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division\n",
      "\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# use seaborn plotting style defaults\n",
      "import seaborn as sns; sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introducing Principal Component Analysis\n",
      "\n",
      "Principal Component Analysis is a very powerful unsupervised method for *dimensionality reduction* in data.  It's easiest to visualize by looking at a two-dimensional dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
      "plt.plot(X[:, 0], X[:, 1], 'o')\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that there is a definite trend in the data. What PCA seeks to do is to find the **Principal Axes** in the data, and explain how important those axes are in describing the data distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "print(pca.explained_variance_ratio_)\n",
      "print(pca.components_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see what these numbers mean, let's view them as vectors plotted on top of the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",