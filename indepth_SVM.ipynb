
{
 "metadata": {
  "name": "",
  "signature": "sha256:c63557555e0a4e14924e8e1d9a88e798cc1143102cf0c93e9af353571250919a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2015. Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2015/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Supervised Learning In-Depth: Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously we introduced supervised machine learning.\n",
      "There are many supervised learning algorithms available; here we'll go into brief detail one of the most powerful and interesting methods: **Support Vector Machines (SVMs)**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# use seaborn plotting defaults\n",
      "import seaborn as sns; sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Motivating Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for **classification** or for **regression**. SVMs are a **discriminative** classifier: that is, they draw a boundary between clusters of data.\n",
      "\n",
      "Let's show a quick example of support vector classification. First we need to create a dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "X, y = make_blobs(n_samples=50, centers=2,\n",
      "                  random_state=0, cluster_std=0.60)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A discriminative classifier attempts to draw a line between the two sets of data. Immediately we see a problem: such a line is ill-posed! For example, we could come up with several possibilities which perfectly discriminate between the classes in this example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xfit = np.linspace(-1, 3.5)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
      "\n",
      "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
      "    plt.plot(xfit, m * xfit + b, '-k')\n",
      "\n",
      "plt.xlim(-1, 3.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are three *very* different separaters which perfectly discriminate between these samples. Depending on which you choose, a new data point will be classified almost entirely differently!\n",
      "\n",
      "How can we improve on this?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Support Vector Machines: Maximizing the *Margin*\n",
      "\n",
      "Support vector machines are one way to address this.\n",
      "What support vector machined do is to not only draw a line, but consider a *region* about the line of some given width.  Here's an example of what it might look like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xfit = np.linspace(-1, 3.5)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
      "\n",
      "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
      "    yfit = m * xfit + b\n",
      "    plt.plot(xfit, yfit, '-k')\n",
      "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
      "\n",
      "plt.xlim(-1, 3.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice here that if we want to maximize this width, the middle fit is clearly the best.\n",
      "This is the intuition of **support vector machines**, which optimize a linear discriminant model in conjunction with a **margin** representing the perpendicular distance between the datasets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Fitting a Support Vector Machine\n",
      "\n",
      "Now we'll fit a Support Vector Machine Classifier to these points. While the mathematical details of the likelihood model are interesting, we'll let you read about those elsewhere. Instead, we'll just treat the scikit-learn algorithm as a black box which accomplishes the above task."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
      "clf = SVC(kernel='linear')\n",
      "clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
     ]
    },
    {