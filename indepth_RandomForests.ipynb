
{
 "metadata": {
  "name": "",
  "signature": "sha256:1cfbdbd1a84e1e50abb22003cd560039a56bb0fb5ab295943913b86a53fd0a61"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2015. Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2015/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Supervised Learning In-Depth: Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously we saw a powerful discriminative classifier, **Support Vector Machines**.\n",
      "Here we'll take a look at motivating another powerful algorithm. This one is a *non-parametric* algorithm called **Random Forests**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# use seaborn plotting defaults\n",
      "import seaborn as sns; sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Motivating Random Forests: Decision Trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random forests are an example of an *ensemble learner* built on decision trees.\n",
      "For this reason we'll start by discussing decision trees themselves.\n",
      "\n",
      "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import fig_code\n",
      "fig_code.plot_example_decision_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The binary splitting makes this extremely efficient.\n",
      "As always, though, the trick is to *ask the right questions*.\n",
      "This is where the algorithmic process comes in: in training a decision tree classifier, the algorithm looks at the features and decides which questions (or \"splits\") contain the most information.\n",
      "\n",
      "### Creating a Decision Tree\n",
      "\n",
      "Here's an example of a decision tree classifier in scikit-learn. We'll start by defining some two-dimensional labeled data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_blobs\n",